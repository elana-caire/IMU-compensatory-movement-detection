{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007ff5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cec7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE_MS = None\n",
    "\n",
    "# We can modify these flags if we want to omit specific modalities from a given sensor (e.g: only IMU)\n",
    "exclude_quat = False\n",
    "exclude_acc = False\n",
    "exclude_gyro = False\n",
    "exclude_mag = False\n",
    "# We can modify this list if we want to omit some sensors\n",
    "sensors_to_consider = [\"arm_l\", \"arm_r\", \"wrist_l\", \"wrist_r\", \"trunk\"]\n",
    "# We can modify this list if we want to omit specific features\n",
    "time_features = [\"MAX\", \"MIN\", \"AMP\", \"MEAN\", \"JERK\", \"RMS\", \"COR\", \"STD\"]\n",
    "frequency_features = [\"DOMFREQ\", \"DOMPOW\", \"TOTPOW\", \"SPEC_CENT\", \"SPEC_SPREAD\"]\n",
    "# Change This flag if we want to apply PCA (otherwise, we can also manually select the features)\n",
    "apply_pca = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cae46c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: if we consider everything we end up with\n",
    "# 3 (acc, gyro, mag) * 3 (axis) * 5 (sensors) * (8 (time features) + 5 (frequency_features)) + 4 (quat) * 5 * 13 --> 845 features!\n",
    "# Which are definently too many for our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f19602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_feature_columns(df, sensors_to_consider, time_features:list, frequency_features:list):\n",
    "    import itertools\n",
    "    feat_columns = []\n",
    "    for sensor in sensors_to_consider:\n",
    "        if time_features is not None:\n",
    "            for time_feat in time_features:\n",
    "                quaternion_columns = (\n",
    "                    df.columns[df.columns.str.contains(\"Quat\") & df.columns.str.contains(sensor) & df.columns.str.contains(time_feat)]\n",
    "                    if not exclude_quat else []\n",
    "                )\n",
    "                acc_columns = (\n",
    "                    df.columns[df.columns.str.contains(\"Acc\") & df.columns.str.contains(sensor) & df.columns.str.contains(time_feat) ]\n",
    "                    if not exclude_acc else []\n",
    "                )\n",
    "                gyr_columns = (\n",
    "                    df.columns[df.columns.str.contains(\"Gyr\") & df.columns.str.contains(sensor) & df.columns.str.contains(time_feat) ]\n",
    "                    if not exclude_gyro else []\n",
    "                )\n",
    "                mag_columns = (\n",
    "                    df.columns[df.columns.str.contains(\"Mag\")& df.columns.str.contains(sensor) & df.columns.str.contains(time_feat)]\n",
    "                    if not exclude_mag else []\n",
    "                )\n",
    "                feat_columns.append(list(quaternion_columns) + list(acc_columns) + list(gyr_columns) + list(mag_columns))\n",
    "        if frequency_features is not None:\n",
    "            for freq_feat in frequency_features:\n",
    "                quaternion_columns = (\n",
    "                    df.columns[df.columns.str.contains(\"Quat\") & df.columns.str.contains(sensor) & df.columns.str.contains(freq_feat)]\n",
    "                    if not exclude_quat else []\n",
    "                )\n",
    "                acc_columns = (\n",
    "                    df.columns[df.columns.str.contains(\"Acc\") & df.columns.str.contains(sensor) & df.columns.str.contains(freq_feat) ]\n",
    "                    if not exclude_acc else []\n",
    "                )\n",
    "                gyr_columns = (\n",
    "                    df.columns[df.columns.str.contains(\"Gyr\") & df.columns.str.contains(sensor) & df.columns.str.contains(freq_feat) ]\n",
    "                    if not exclude_gyro else []\n",
    "                )\n",
    "                mag_columns = (\n",
    "                    df.columns[df.columns.str.contains(\"Mag\")& df.columns.str.contains(sensor) & df.columns.str.contains(freq_feat)]\n",
    "                    if not exclude_mag else []\n",
    "                )\n",
    "                feat_columns.append(list(quaternion_columns) + list(acc_columns) + list(gyr_columns) + list(mag_columns))\n",
    "        \n",
    "\n",
    "\n",
    "    return list(itertools.chain.from_iterable(feat_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a432683",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features_path = r\"C:\\Users\\giusy\\OneDrive\\Desktop\\AI_Healtcare\\imu_compensatory_movements\\Data\\Features\"\n",
    "save_models_path = r\"C:\\Users\\giusy\\OneDrive\\Desktop\\AI_Healtcare\\imu_compensatory_movements\\Data\\Models\"\n",
    "if WINDOW_SIZE_MS is not None:\n",
    "    df_features = pd.read_csv(save_features_path+f\"/features_win_{WINDOW_SIZE_MS}.csv\")\n",
    "    results_save_path = save_models_path+ \"/loso_hyperparam_window_{WINDOW_SIZE_MS}.csv\"\n",
    "else:\n",
    "    df_features = pd.read_csv(save_features_path+f\"/features.csv\")\n",
    "    results_save_path = save_models_path+ \"/loso_hyperparam.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23fff1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add one-hot encoding for conditions\n",
    "df_features.loc[:,'Label'] = -1\n",
    "\n",
    "mask = df_features['condition'] == 'natural'\n",
    "df_features.loc[mask, 'Label'] = 0\n",
    "mask = df_features['condition'] == 'elbow_brace'\n",
    "df_features.loc[mask, 'Label'] = 1\n",
    "mask = df_features['condition'] == 'elbow_wrist_brace'\n",
    "df_features.loc[mask, 'Label'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "267b7be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5f97f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks: ['cup-placing' 'peg' 'wiping' 'pouring']\n"
     ]
    }
   ],
   "source": [
    "tasks = df_features[\"task\"].unique()\n",
    "print(\"Tasks:\", tasks)\n",
    "# consider feature columns according to specifications\n",
    "feat_cols = return_feature_columns(df_features, sensors_to_consider=sensors_to_consider, time_features=time_features, frequency_features=frequency_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c53c68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b93a7bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with task: cup-placing\n",
      "Done with task: peg\n",
      "Done with task: wiping\n",
      "Done with task: pouring\n",
      "Done with task: cup-placing\n",
      "Done with task: peg\n",
      "Done with task: wiping\n",
      "Done with task: pouring\n",
      "Done with task: cup-placing\n",
      "Done with task: peg\n",
      "Done with task: wiping\n",
      "Done with task: pouring\n",
      "Done with task: cup-placing\n",
      "Done with task: peg\n",
      "Done with task: wiping\n",
      "Done with task: pouring\n",
      "Done with task: cup-placing\n",
      "Done with task: peg\n",
      "Done with task: wiping\n",
      "Done with task: pouring\n"
     ]
    }
   ],
   "source": [
    "# Outher-loop: Leave one subject out\n",
    "subject_ids = ['P02', 'P03','P04', 'P05', 'P06']\n",
    "\n",
    "for test_subject in subject_ids:\n",
    "    df_subj_train = df_features[df_features[\"subject\"]!=test_subject]\n",
    "    df_subj_test = df_features[df_features[\"subject\"]==test_subject]\n",
    "\n",
    "    # now, split trough all the conditions\n",
    "    for task in tasks:\n",
    "        df_train = df_subj_train[df_subj_train[\"task\"] == task]\n",
    "        df_test = df_subj_test[df_subj_test[\"task\"] == task]\n",
    "\n",
    "        X_train = df_train[feat_cols]\n",
    "        Y_train = df_train['Label']\n",
    "\n",
    "        X_test = df_train[feat_cols]\n",
    "        Y_test = df_train['Label']\n",
    "\n",
    "\n",
    "        # Initialize the scaler\n",
    "        scl = StandardScaler()\n",
    "        # Fit the scaler to the Training Data\n",
    "        scl.fit(X_train)\n",
    "        X_train_scaled = scl.transform(X_train)\n",
    "        X_test_scaled = scl.transform(X_test)\n",
    "\n",
    "\n",
    "        ## Optional : apply PCA --> to add\n",
    "\n",
    "        \n",
    "        # Initialize the model \n",
    "\n",
    "        print(\"Done with task:\", task)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b1cdf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\giusy\\OneDrive\\Desktop\\AI_Healtcare\\imu_compensatory_movements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3feaeb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4add373a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m subject_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP02\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP03\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP04\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP05\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP06\u001b[39m\u001b[38;5;124m'\u001b[39m]   \u001b[38;5;66;03m# or sorted(df_features[\"subject\"].unique())\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# tasks already defined somewhere, e.g.:\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# tasks = sorted(df_features[\"task\"].unique())\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, (estimator, param_grid) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import f1_score  # or accuracy_score, etc.\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "subject_ids = ['P02', 'P03', 'P04', 'P05', 'P06']   # or sorted(df_features[\"subject\"].unique())\n",
    "# tasks already defined somewhere, e.g.:\n",
    "# tasks = sorted(df_features[\"task\"].unique())\n",
    "\n",
    "for model_name, (estimator, param_grid) in models.items():\n",
    "    print(f\"\\n=== Model: {model_name} ===\")\n",
    "    if model_name == 'XGBoost':\n",
    "        best_params = None\n",
    "        best_score = -np.inf\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # OUTER LOOP: iterate over all hyperparameter combinations\n",
    "        # -------------------------------------------------------\n",
    "        for params in ParameterGrid(param_grid):\n",
    "            print(f\"\\nTesting params: {params}\")\n",
    "            scores = []\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # INNER LOOP: LOSO over subjects (and tasks inside)\n",
    "            # ---------------------------------------------------\n",
    "            for test_subject in subject_ids:\n",
    "                df_subj_train = df_features[df_features[\"subject\"] != test_subject]\n",
    "                df_subj_test  = df_features[df_features[\"subject\"] == test_subject]\n",
    "\n",
    "                for task in tasks:\n",
    "                    df_train = df_subj_train[df_subj_train[\"task\"] == task]\n",
    "                    df_test  = df_subj_test[df_subj_test[\"task\"] == task]\n",
    "\n",
    "\n",
    "                    X_train = df_train[feat_cols].values\n",
    "                    y_train = df_train['Label'].values\n",
    "\n",
    "                    # IMPORTANT: use df_test here, not df_train\n",
    "                    X_test = df_test[feat_cols].values\n",
    "                    y_test = df_test['Label'].values\n",
    "\n",
    "                    # Standardize (fit only on train)\n",
    "                    scl = StandardScaler()\n",
    "                    X_train_scaled = scl.fit_transform(X_train)\n",
    "                    X_test_scaled  = scl.transform(X_test)\n",
    "\n",
    "                    # Optional PCA\n",
    "                    if apply_pca:\n",
    "                        print(\"Applying PCA (TODO)\")\n",
    "                        # add PCA here if you want\n",
    "\n",
    "                    # Clone base estimator and set current params\n",
    "                    clf = clone(estimator).set_params(**params)\n",
    "                    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "                    y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "                    # Example metric: macro F1 across classes\n",
    "                    score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "                    scores.append(score)\n",
    "\n",
    "            if not scores:\n",
    "                print(\"No valid splits for these params (maybe empty tasks), skipping\")\n",
    "                continue\n",
    "\n",
    "            mean_score = np.mean(scores)\n",
    "            print(f\"Mean LOSO-CV score for {params}: {mean_score:.4f}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9208b69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ae46cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04b6b17f",
   "metadata": {},
   "source": [
    "# Analyize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c78134",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(r\"C:\\Users\\giusy\\OneDrive\\Desktop\\AI_Healtcare\\imu_compensatory_movements\\Data\\Models\\loso_hyperparam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ee0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>estimator_type</th>\n",
       "      <th>params</th>\n",
       "      <th>test_subject</th>\n",
       "      <th>task</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'n_esti...</td>\n",
       "      <td>P02</td>\n",
       "      <td>cup-placing</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'n_esti...</td>\n",
       "      <td>P02</td>\n",
       "      <td>peg</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'n_esti...</td>\n",
       "      <td>P02</td>\n",
       "      <td>wiping</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'n_esti...</td>\n",
       "      <td>P02</td>\n",
       "      <td>pouring</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 3, 'n_esti...</td>\n",
       "      <td>P03</td>\n",
       "      <td>cup-placing</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>RF</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 200}</td>\n",
       "      <td>P05</td>\n",
       "      <td>pouring</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>RF</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 200}</td>\n",
       "      <td>P06</td>\n",
       "      <td>cup-placing</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>RF</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 200}</td>\n",
       "      <td>P06</td>\n",
       "      <td>peg</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>RF</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 200}</td>\n",
       "      <td>P06</td>\n",
       "      <td>wiping</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>RF</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 200}</td>\n",
       "      <td>P06</td>\n",
       "      <td>pouring</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_name          estimator_type  \\\n",
       "0      XGBoost           XGBClassifier   \n",
       "1      XGBoost           XGBClassifier   \n",
       "2      XGBoost           XGBClassifier   \n",
       "3      XGBoost           XGBClassifier   \n",
       "4      XGBoost           XGBClassifier   \n",
       "..         ...                     ...   \n",
       "335         RF  RandomForestClassifier   \n",
       "336         RF  RandomForestClassifier   \n",
       "337         RF  RandomForestClassifier   \n",
       "338         RF  RandomForestClassifier   \n",
       "339         RF  RandomForestClassifier   \n",
       "\n",
       "                                                params test_subject  \\\n",
       "0    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...          P02   \n",
       "1    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...          P02   \n",
       "2    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...          P02   \n",
       "3    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...          P02   \n",
       "4    {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...          P03   \n",
       "..                                                 ...          ...   \n",
       "335             {'max_depth': 20, 'n_estimators': 200}          P05   \n",
       "336             {'max_depth': 20, 'n_estimators': 200}          P06   \n",
       "337             {'max_depth': 20, 'n_estimators': 200}          P06   \n",
       "338             {'max_depth': 20, 'n_estimators': 200}          P06   \n",
       "339             {'max_depth': 20, 'n_estimators': 200}          P06   \n",
       "\n",
       "            task  f1_macro  accuracy  precision_macro  recall_macro  \\\n",
       "0    cup-placing  0.555556  0.666667              0.5      0.666667   \n",
       "1            peg  0.555556  0.666667              0.5      0.666667   \n",
       "2         wiping  0.555556  0.666667              0.5      0.666667   \n",
       "3        pouring  0.555556  0.666667              0.5      0.666667   \n",
       "4    cup-placing  1.000000  1.000000              1.0      1.000000   \n",
       "..           ...       ...       ...              ...           ...   \n",
       "335      pouring  1.000000  1.000000              1.0      1.000000   \n",
       "336  cup-placing  0.555556  0.666667              0.5      0.666667   \n",
       "337          peg  0.555556  0.666667              0.5      0.666667   \n",
       "338       wiping  1.000000  1.000000              1.0      1.000000   \n",
       "339      pouring  1.000000  1.000000              1.0      1.000000   \n",
       "\n",
       "                                              features  \n",
       "0    ['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...  \n",
       "1    ['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...  \n",
       "2    ['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...  \n",
       "3    ['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...  \n",
       "4    ['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...  \n",
       "..                                                 ...  \n",
       "335  ['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...  \n",
       "336  ['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...  \n",
       "337  ['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...  \n",
       "338  ['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...  \n",
       "339  ['MAX', 'MIN', 'AMP', 'MEAN', 'JERK', 'RMS', '...  \n",
       "\n",
       "[340 rows x 10 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24870762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 1.        , 0.33333333])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"accuracy\"].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_health",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
