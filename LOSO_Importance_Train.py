
import warnings
warnings.filterwarnings('ignore')
import joblib
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
from utils.ml import *
from config import CONFIG

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier

from sklearn.inspection import permutation_importance  # <-- NEW

PERM_N_REPEATS = 10          # <-- NEW (tune as needed)
PERM_SCORING  = "f1_macro"   # <-- NEW (or "accuracy")


RANDOM_STATE = 42

# these are for shap
BG_N = 100
EXPL_N = 50

best_models = ["LASSO_LR", "MLP"]                   # THESE ARE THE BEST MODELS


def parse_params_cell(v):
    import json
    import ast
    if isinstance(v, dict):
        return v
    if v is None or (isinstance(v, float) and pd.isna(v)):
        return {}

    s = str(v).strip()

    # strip one outer quote layer if present
    if len(s) >= 2 and s[0] == s[-1] and s[0] in ("'", '"'):
        s = s[1:-1].strip()

    # unescape if needed
    s = s.encode("utf-8").decode("unicode_escape").strip()

    # handle case like:  'C':'10'  (missing braces)
    if not s.lstrip().startswith("{"):
        s = "{" + s.strip().strip(",") + "}"

    try:
        d = json.loads(s)
    except json.JSONDecodeError:
        d = ast.literal_eval(s)

    if not isinstance(d, dict):
        raise ValueError(f"Parsed value is not a dict: {type(d)}")

    return d

def shap_to_3d(shap_values):
    """
    Return SHAP values as (n_samples, n_features, n_classes).
    Handles:
      - array (n, f, c)
      - array (n, f) -> assumes single output -> (n, f, 1)
      - list of arrays (per class) each (n, f) -> (n, f, c)
    """
    if isinstance(shap_values, list):
        sv = np.stack(shap_values, axis=-1)  # (n, f, c)
        return sv
    sv = np.asarray(shap_values)
    if sv.ndim == 2:
        return sv[:, :, None]
    if sv.ndim == 3:
        return sv
    raise ValueError(f"Unexpected SHAP shape/type: {type(shap_values)}, shape={getattr(sv,'shape',None)}")



## Parse The Results File Generated by the other script
SAVE_DIR = r"C:\Users\giusy\OneDrive\Desktop\AI_Healtcare\IMU-compensatory-movement-detection\Data\ModelsFinal"
models_summary_path = SAVE_DIR+"/model_results_summary_refactored.csv"          
# this is the file where results summary is saved
task_specific = True               # set to True to Train Task specific models, else false

# ---- choose the tag used in your results file for task-agnostic ----
TASK_AGNOSTIC_TAG = "all_tasks"


# SCRIPT TO RE-RUN SHAP on TOP-PERFORMING models after LOSO
if __name__=='__main__':
    

    # LOAD DATA
    feature_files = find_feature_files(CONFIG["features_folder"])
    for file_path, window_size in tqdm(feature_files, desc="Processing windows"):
        if window_size == "750":
            print("Processing window :", window_size)
            df = pd.read_csv(file_path)
            df = df.loc[:, ~df.columns.str.contains("^Unnamed")]
            df, label_encoder = encode_labels(df, CONFIG["target"])


            #features = [c for c in df.columns if c not in ['subject', 'task', CONFIG["target"], 'Label']]

            ## Consider Experimental Parameters from config.py

            features = return_feature_columns(df, 
                                            sensors_to_consider=CONFIG["sensors_to_consider"], 
                                            time_features=CONFIG["time_features"], 
                                            frequency_features=CONFIG["frequency_features"], 
                                            exclude_acc=CONFIG["exclude_acc"], 
                                            exclude_gyro=CONFIG["exclude_gyro"], 
                                            exclude_mag=CONFIG["exclude_mag"], 
                                            exclude_quat=CONFIG["exclude_quat"])






            results = pd.read_csv(models_summary_path)
            
            # Evaluation metrics
            metrics = ['f1_macro_mean', 'accuracy_mean', 'precision_macro_mean', 'recall_macro_mean', 'roc_auc_mean']

            # Keep only the columns we care about
            results_small = results[['task', 'model_name', 'window_size', 'best_params'] + metrics].copy()

            # ---------- SUMMARY DATAFRAME ----------

            # One row per (task, model_name, window_size), metrics averaged over repeats
            summary = (
                results_small
                .groupby(['task', 'model_name', 'window_size', 'best_params'], as_index=False)[metrics]
                .mean()
                .sort_values(['task', 'model_name', 'window_size'])
            )

            task_names = summary['task'].unique()

            df_best = summary[summary['window_size'] == "750"]
            df_best = df_best[df_best["model_name"].isin(best_models)]

            for task_name in df_best["task"].unique():
                if task_name != "peg":
                    print("Alreadz trained")
                    continue

                is_task_agnostic = (task_name == TASK_AGNOSTIC_TAG)

                # -------------------- DATA SELECTION --------------------
                # task-specific -> filter by task
                # task-agnostic -> use ALL tasks
                data_task = df if is_task_agnostic else df[df["task"] == task_name]
                subjects = data_task["subject"].unique()

                for curr_model in best_models:
                    if curr_model!='MLP':
                        print(f"{curr_model} already trained")
                        continue

                    # -------------------- PARAMS SELECTION --------------------
                    params_task = TASK_AGNOSTIC_TAG if is_task_agnostic else task_name
                    row_sel = df_best[(df_best["task"] == params_task) & (df_best["model_name"] == curr_model)]
                    if row_sel.empty:
                        print(f"[WARN] No params found for task={params_task}, model={curr_model}. Skipping.")
                        continue
                    params_dict = parse_params_cell(row_sel.iloc[0]["best_params"])

                    per_subject_rows = []
                    per_subject_metrics = []
                    per_subject_perm = []

                    for subject in subjects:

                        # LOSO
                        data_task_train = data_task[data_task["subject"] != subject]
                        data_task_test  = data_task[data_task["subject"] == subject]

                        X_train = data_task_train[features]
                        y_train = data_task_train["Label"]
                        X_test  = data_task_test[features]
                        y_test  = data_task_test["Label"]

                        print(f"Retraining model:{curr_model} on task:{task_name} (agnostic={is_task_agnostic})")

                        # init
                        if curr_model == "LASSO_LR":
                            model = LogisticRegression(
                                penalty="l1", solver="saga", multi_class="multinomial",
                                max_iter=1000, random_state=RANDOM_STATE
                            )
                        elif curr_model == "MLP":
                            model = MLPClassifier(random_state=RANDOM_STATE, max_iter=100)
                        else:
                            raise ValueError(curr_model)

                        model.set_params(**params_dict)

                        # scale + fit
                        scaler = StandardScaler()
                        X_train_scaled = scaler.fit_transform(X_train)
                        X_test_scaled  = scaler.transform(X_test)
                        model.fit(X_train_scaled, y_train)

                        # metrics per fold
                        y_pred  = model.predict(X_test_scaled)
                        y_proba = model.predict_proba(X_test_scaled) if hasattr(model, "predict_proba") else None
                        m = compute_metrics(y_test.values, y_pred, y_proba)

                        per_subject_metrics.append({
                            "task": task_name,
                            "model_name": curr_model,
                            "subject": subject,
                            **m
                        })

                        # subsample for SHAP/PI
                        X_bg, _ = balanced_subsample_after_scaling(
                            X_train_scaled, y_train, n_total=BG_N, random_state=RANDOM_STATE
                        )
                        X_to_explain, y_to_explain = balanced_subsample_after_scaling(
                            X_test_scaled, y_test, n_total=EXPL_N, random_state=RANDOM_STATE
                        )

                        # SHAP
                        if curr_model == "LASSO_LR":
                            explainer = shap.LinearExplainer(model, X_bg)
                            shap_values = explainer.shap_values(X_to_explain)
                        else:
                            explainer = shap.KernelExplainer(model.predict_proba, X_bg)
                            shap_values = explainer.shap_values(X_to_explain)

                        sv3 = shap_to_3d(shap_values)         # (n, f, c)
                        imp_fc = np.mean(np.abs(sv3), axis=0) # (f, c)

                        for fi, feat in enumerate(features):
                            for ci in range(imp_fc.shape[1]):
                                per_subject_rows.append({
                                    "task": task_name,
                                    "model_name": curr_model,
                                    "subject": subject,
                                    "feature": feat,
                                    "class": ci,
                                    "mean_abs_shap": float(imp_fc[fi, ci]),
                                })

                        # permutation importance (on X_to_explain/y_to_explain)
                        try:
                            pi = permutation_importance(
                                model,
                                X_to_explain,
                                np.asarray(y_to_explain),
                                scoring=PERM_SCORING,
                                n_repeats=PERM_N_REPEATS,
                                random_state=RANDOM_STATE,
                                n_jobs=-1
                            )
                            for feat, mean_imp, std_imp in zip(features, pi.importances_mean, pi.importances_std):
                                per_subject_perm.append({
                                    "task": task_name,
                                    "model_name": curr_model,
                                    "subject": subject,
                                    "feature": feat,
                                    "perm_importance_mean": float(mean_imp),
                                    "perm_importance_std": float(std_imp),
                                    "scoring": PERM_SCORING,
                                    "n_repeats": PERM_N_REPEATS
                                })
                        except Exception as e:
                            print(f"[WARN] PI failed (task={task_name}, model={curr_model}, subject={subject}): {e}")

                        # save model bundle (optional; naming works for both cases)
                        model_out_dir = os.path.join(SAVE_DIR, "saved_models", task_name, curr_model)
                        os.makedirs(model_out_dir, exist_ok=True)
                        model_path = os.path.join(model_out_dir, f"{curr_model}_task_{task_name}_test_subject_{subject}.joblib")

                        joblib.dump({
                            "model": model,
                            "scaler": scaler,
                            "features": features,
                            "task": task_name,
                            "model_name": curr_model,
                            "test_subject": subject,
                            "window_size": window_size,
                            "params": params_dict,
                            "label_encoder": label_encoder
                        }, model_path)

                    # ---------- SAVE outputs (same filenames; task_name will be "all_tasks" for agnostic) ----------
                    df_shap_subj = pd.DataFrame(per_subject_rows)
                    df_shap_subj.to_csv(os.path.join(SAVE_DIR, f"shap_{task_name}_{curr_model}_test.csv"), index=False)

                    df_shap_agg = (
                        df_shap_subj.groupby(["task","model_name","feature","class"], as_index=False)["mean_abs_shap"]
                                    .agg(["mean","std"])
                                    .reset_index()
                                    .rename(columns={"mean":"mean_abs_shap_mean", "std":"mean_abs_shap_std"})
                    )
                    df_shap_agg.to_csv(os.path.join(SAVE_DIR, f"shap_{task_name}_{curr_model}_mean_std.csv"), index=False)

                    df_perm_subj = pd.DataFrame(per_subject_perm)
                    df_perm_subj.to_csv(os.path.join(SAVE_DIR, f"perm_{task_name}_{curr_model}_test.csv"), index=False)

                    df_perm_agg = (
                        df_perm_subj.groupby(["task","model_name","feature"], as_index=False)["perm_importance_mean"]
                                    .agg(["mean","std"])
                                    .reset_index()
                                    .rename(columns={"mean":"perm_importance_mean_mean", "std":"perm_importance_mean_std"})
                    )
                    df_perm_agg.to_csv(os.path.join(SAVE_DIR, f"perm_{task_name}_{curr_model}_mean_std.csv"), index=False)

                    df_metrics_subj = pd.DataFrame(per_subject_metrics)
                    metric_cols = [c for c in df_metrics_subj.columns if c not in ["task","model_name","subject"]]
                    df_metrics_agg = (
                        df_metrics_subj.groupby(["task","model_name"], as_index=False)[metric_cols]
                                    .agg(["mean","std"]).reset_index()
                    )
                    df_metrics_agg.columns = [
                        f"{a}_{b}" if b else a for a, b in df_metrics_agg.columns.to_flat_index()
                    ]
                    df_metrics_agg.to_csv(os.path.join(SAVE_DIR, f"metrics_{task_name}_{curr_model}_mean_std.csv"), index=False)