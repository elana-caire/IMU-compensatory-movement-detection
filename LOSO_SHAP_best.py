# Interesting Article Here: https://www.linkedin.com/pulse/shap-all-you-need-why-should-always-use-permutation-alastair-olunc/
import warnings
warnings.filterwarnings('ignore')
import joblib
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
from utils.ml import *
from config import CONFIG

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier

from sklearn.inspection import permutation_importance  # <-- NEW

PERM_N_REPEATS = 10          # <-- NEW (tune as needed)
PERM_SCORING  = "f1_macro"   # <-- NEW (or "accuracy")


RANDOM_STATE = 42

# these are for shap
BG_N = 100
EXPL_N = 50

best_models = ["LASSO_LR", "MLP"]                   # THESE ARE THE BEST MODELS


def parse_params_cell(v):
    import json
    import ast
    if isinstance(v, dict):
        return v
    if v is None or (isinstance(v, float) and pd.isna(v)):
        return {}

    s = str(v).strip()

    # strip one outer quote layer if present
    if len(s) >= 2 and s[0] == s[-1] and s[0] in ("'", '"'):
        s = s[1:-1].strip()

    # unescape if needed
    s = s.encode("utf-8").decode("unicode_escape").strip()

    # handle case like:  'C':'10'  (missing braces)
    if not s.lstrip().startswith("{"):
        s = "{" + s.strip().strip(",") + "}"

    try:
        d = json.loads(s)
    except json.JSONDecodeError:
        d = ast.literal_eval(s)

    if not isinstance(d, dict):
        raise ValueError(f"Parsed value is not a dict: {type(d)}")

    return d

def shap_to_3d(shap_values):
    """
    Return SHAP values as (n_samples, n_features, n_classes).
    Handles:
      - array (n, f, c)
      - array (n, f) -> assumes single output -> (n, f, 1)
      - list of arrays (per class) each (n, f) -> (n, f, c)
    """
    if isinstance(shap_values, list):
        sv = np.stack(shap_values, axis=-1)  # (n, f, c)
        return sv
    sv = np.asarray(shap_values)
    if sv.ndim == 2:
        return sv[:, :, None]
    if sv.ndim == 3:
        return sv
    raise ValueError(f"Unexpected SHAP shape/type: {type(shap_values)}, shape={getattr(sv,'shape',None)}")



## Parse The Results File Generated by the other script
SAVE_DIR = r"C:\Users\giusy\OneDrive\Desktop\AI_Healtcare\IMU-compensatory-movement-detection\Data\ModelsFinal"
models_summary_path = SAVE_DIR+"/model_results_summary_refactored.csv"          


# SCRIPT TO RE-RUN SHAP on TOP-PERFORMING models after LOSO
if __name__=='__main__':
    

    # LOAD DATA
    feature_files = find_feature_files(CONFIG["features_folder"])
    for file_path, window_size in tqdm(feature_files, desc="Processing windows"):
        if window_size == "750":
            print("Processing window :", window_size)
            df = pd.read_csv(file_path)
            df = df.loc[:, ~df.columns.str.contains("^Unnamed")]
            df, label_encoder = encode_labels(df, CONFIG["target"])


            #features = [c for c in df.columns if c not in ['subject', 'task', CONFIG["target"], 'Label']]

            ## Consider Experimental Parameters from config.py

            features = return_feature_columns(df, 
                                            sensors_to_consider=CONFIG["sensors_to_consider"], 
                                            time_features=CONFIG["time_features"], 
                                            frequency_features=CONFIG["frequency_features"], 
                                            exclude_acc=CONFIG["exclude_acc"], 
                                            exclude_gyro=CONFIG["exclude_gyro"], 
                                            exclude_mag=CONFIG["exclude_mag"], 
                                            exclude_quat=CONFIG["exclude_quat"])






            results = pd.read_csv(models_summary_path)
            
            # Evaluation metrics
            metrics = ['f1_macro_mean', 'accuracy_mean', 'precision_macro_mean', 'recall_macro_mean', 'roc_auc_mean']

            # Keep only the columns we care about
            results_small = results[['task', 'model_name', 'window_size', 'best_params'] + metrics].copy()

            # ---------- SUMMARY DATAFRAME ----------

            # One row per (task, model_name, window_size), metrics averaged over repeats
            summary = (
                results_small
                .groupby(['task', 'model_name', 'window_size', 'best_params'], as_index=False)[metrics]
                .mean()
                .sort_values(['task', 'model_name', 'window_size'])
            )

            task_names = summary['task'].unique()

            df_best = summary[summary['window_size'] == "750"]
            df_best = df_best[df_best["model_name"].isin(best_models)]

            if task_specific:
                for task_name in df_best["task"].unique():
                    

                    data_task = df[df["task"] == task_name]
                    subjects = data_task["subject"].unique()

                    for curr_model in best_models:

                        row = df_best[(df_best["task"] == task_name) & (df_best["model_name"] == curr_model)].iloc[0]
                        params_dict = parse_params_cell(row["best_params"])

                        per_subject_rows = []
                        per_subject_metrics = []  # <-- NEW: store 1 metrics dict per (subject fold)
                        per_subject_perm = []  # <-- NEW: permutation importance per (subject, feature)

                        for subject in subjects:

                            # LOSO: train on others, test on left-out subject
                            data_task_train = data_task[data_task["subject"] != subject]
                            data_task_test  = data_task[data_task["subject"] == subject]

                            X_train = data_task_train[features]
                            y_train = data_task_train["Label"]
                            X_test  = data_task_test[features]
                            y_test  = data_task_test["Label"]

                            print(f"Retraining model:{curr_model} on task:{task_name}")
                            # model init
                            if curr_model == "LASSO_LR":
                                model = LogisticRegression(
                                    penalty="l1", solver="saga", multi_class="multinomial",
                                    max_iter=1000, random_state=RANDOM_STATE
                                )
                            elif curr_model == "MLP":
                                model = MLPClassifier(random_state=RANDOM_STATE, max_iter=100)
                            else:
                                raise ValueError(curr_model)

                            model.set_params(**params_dict)

                            # scale + fit
                            scaler = StandardScaler()
                            X_train_scaled = scaler.fit_transform(X_train)
                            X_test_scaled  = scaler.transform(X_test)

                            model.fit(X_train_scaled, y_train)

                            # ---------- METRICS (per LOSO fold) ----------  # <-- NEW
                            y_pred = model.predict(X_test_scaled)  # <-- NEW
                            y_proba = model.predict_proba(X_test_scaled) if hasattr(model, "predict_proba") else None  # <-- NEW
                            m = compute_metrics(y_test.values, y_pred, y_proba)  # <-- NEW

                            per_subject_metrics.append({  # <-- NEW
                                "task": task_name,
                                "model_name": curr_model,
                                "subject": subject,
                                **m
                            })
                            print(f"F1_macro={m['f1_macro']:.3f}, Acc={m['accuracy']:.3f}")




                            # balanced background + explain sets (AFTER scaling)
                            X_bg, _ = balanced_subsample_after_scaling(X_train_scaled, y_train, n_total=BG_N,  random_state=RANDOM_STATE)
                            X_to_explain, y_to_explain = balanced_subsample_after_scaling(X_test_scaled,  y_test,  n_total=EXPL_N, random_state=RANDOM_STATE)

                            # --- SHAP ---
                            if curr_model == "LASSO_LR":
                                # much faster + exact for linear models
                                explainer = shap.LinearExplainer(model, X_bg)
                                shap_values = explainer.shap_values(X_to_explain)
                            else:
                                # model-agnostic (slower)
                                explainer = shap.KernelExplainer(model.predict_proba, X_bg)
                                shap_values = explainer.shap_values(X_to_explain)

                            sv3 = shap_to_3d(shap_values)              # (n, f, c)
                            imp_fc = np.mean(np.abs(sv3), axis=0)      # (f, c) mean(|SHAP|) per feature per class

                            # store long format: subject x feature x class
                            for fi, feat in enumerate(features):
                                for ci in range(imp_fc.shape[1]):
                                    per_subject_rows.append({
                                        "task": task_name,
                                        "model_name": curr_model,
                                        "subject": subject,
                                        "feature": feat,
                                        "class": ci,
                                        "mean_abs_shap": float(imp_fc[fi, ci]),
                                    })
                            

                            # ---------- PERMUTATION IMPORTANCE (per LOSO fold) ----------  # <-- NEW
                            try:
                                pi = permutation_importance(
                                    model,
                                    X_to_explain,
                                    np.asarray(y_to_explain),
                                    scoring=PERM_SCORING,
                                    n_repeats=PERM_N_REPEATS,
                                    random_state=RANDOM_STATE,
                                    n_jobs=-1
                                )

                                # store long format: subject x feature
                                for feat, mean_imp, std_imp in zip(features, pi.importances_mean, pi.importances_std):
                                    per_subject_perm.append({
                                        "task": task_name,
                                        "model_name": curr_model,
                                        "subject": subject,
                                        "feature": feat,
                                        "perm_importance_mean": float(mean_imp),
                                        "perm_importance_std": float(std_imp),
                                        "scoring": PERM_SCORING,
                                        "n_repeats": PERM_N_REPEATS
                                    })

                            except Exception as e:
                                print(f"[WARN] Permutation importance failed (task={task_name}, model={curr_model}, subject={subject}): {e}")
                                
                            # ---- after training ----
                            model_out_dir = os.path.join(SAVE_DIR, "saved_models", task_name, curr_model)
                            os.makedirs(model_out_dir, exist_ok=True)

                            model_path = os.path.join(
                                model_out_dir,
                                f"{curr_model}_task_{task_name}_test_subject_{subject}.joblib"
                            )

                            bundle = {
                                "model": model,
                                "scaler": scaler,
                                "features": features,          # important: column order
                                "task": task_name,
                                "model_name": curr_model,
                                "test_subject": subject,
                                "window_size": window_size,    # if you have it in scope, else set "750"
                                "params": params_dict,
                                "label_encoder": label_encoder # optional but useful for mapping classes
                            }

                            joblib.dump(bundle, model_path)
                            print(f"[Saved model bundle] {model_path}")

                        #--------------------------SAVE SHAP METRICS ------------------------------#
                        df_shap_subj = pd.DataFrame(per_subject_rows)

                        # save per-task per-model (per-subject values)
                        out_csv = os.path.join(SAVE_DIR, f"shap_{task_name}_{curr_model}_test.csv")
                        df_shap_subj.to_csv(out_csv, index=False)

                        # also compute mean/std across subjects (what you want for plotting)
                        df_shap_agg = (
                            df_shap_subj.groupby(["task","model_name","feature","class"], as_index=False)["mean_abs_shap"]
                                        .agg(["mean","std"])
                                        .reset_index()
                                        .rename(columns={"mean":"mean_abs_shap_mean", "std":"mean_abs_shap_std"})
                        )

                        out_csv2 = os.path.join(SAVE_DIR, f"shap_{task_name}_{curr_model}_mean_std.csv")
                        df_shap_agg.to_csv(out_csv2, index=False)

                        print(f"[Saved] {out_csv}")
                        print(f"[Saved] {out_csv2}")

                        # ---------- SAVE PERMUTATION IMPORTANCE (per subject) ----------  # <-- NEW
                        df_perm_subj = pd.DataFrame(per_subject_perm)  # <-- NEW
                        perm_out_csv = os.path.join(SAVE_DIR, f"perm_{task_name}_{curr_model}_test.csv")  # <-- NEW
                        df_perm_subj.to_csv(perm_out_csv, index=False)  # <-- NEW

                        # ---------- SAVE PERMUTATION IMPORTANCE (mean/std across subjects) ----------  # <-- NEW
                        df_perm_agg = (
                            df_perm_subj
                            .groupby(["task", "model_name", "feature"], as_index=False)["perm_importance_mean"]
                            .agg(["mean", "std"])
                            .reset_index()
                            .rename(columns={"mean": "perm_importance_mean_mean", "std": "perm_importance_mean_std"})
                        )  # <-- NEW

                        perm_out_csv2 = os.path.join(SAVE_DIR, f"perm_{task_name}_{curr_model}_mean_std.csv")  # <-- NEW
                        df_perm_agg.to_csv(perm_out_csv2, index=False)  # <-- NEW

                        print(f"[Saved] {perm_out_csv}")   # <-- NEW
                        print(f"[Saved] {perm_out_csv2}")  # <-- NEW

                        # ---------- SAVE METRICS (mean/std across subjects) ----------  # <-- NEW
                        df_metrics_subj = pd.DataFrame(per_subject_metrics)  # <-- NEW
                        metric_cols = [c for c in df_metrics_subj.columns if c not in ["task", "model_name", "subject"]]  # <-- NEW

                        df_metrics_agg = (
                            df_metrics_subj
                            .groupby(["task", "model_name"], as_index=False)[metric_cols]
                            .agg(["mean", "std"])
                            .reset_index()
                        )  # <-- NEW

                        # flatten multiindex columns (e.g., accuracy_mean, accuracy_std)  # <-- NEW
                        df_metrics_agg.columns = [
                            f"{a}_{b}" if b else a
                            for a, b in df_metrics_agg.columns.to_flat_index()
                        ]  # <-- NEW

                        metrics_out_csv2 = os.path.join(SAVE_DIR, f"metrics_{task_name}_{curr_model}_mean_std.csv")  # <-- NEW
                        df_metrics_agg.to_csv(metrics_out_csv2, index=False)  # <-- NEW


